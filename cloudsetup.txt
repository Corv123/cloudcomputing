# AWS Cloud Setup Guide for Fake News Detector

## Prerequisites
- AWS Account with appropriate permissions
- AWS CLI installed and configured
- Python 3.11+ environment
- Docker (for ML containerization)

## Phase 1: Frontend Setup (S3 + CloudFront)

### Step 1: Create S3 Bucket
```bash
# Create bucket for static website
aws s3 mb s3://fakenews-detector-frontend

# Enable static website hosting
aws s3 website s3://fakenews-detector-frontend --index-document index.html --error-document error.html
```

### Step 2: Upload Frontend Files
```bash
# Upload static files to S3
aws s3 sync fakenews/static/ s3://fakenews-detector-frontend --delete

# Set proper permissions
# PowerShell-safe: write JSON to a file, then apply policy
$policy = @"
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicReadGetObject",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::fakenews-detector-frontend/*"
    }
  ]
}
"@
$policy | Out-File -FilePath policy.json -Encoding ascii
aws s3api put-bucket-policy --bucket fakenews-detector-frontend --policy file://policy.json --region ap-southeast-2
```

### Step 3: Setup CloudFront Distribution
```bash
# Create CloudFront distribution
# Recommended PowerShell flow with CloudFront OAC (keep S3 bucket private)
# 1) Create OAC
$oac = aws cloudfront create-origin-access-control --origin-access-control-config "{\"Name\":\"fakenews-oac-frontend\",\"Description\":\"OAC for frontend S3\",\"SigningProtocol\":\"sigv4\",\"SigningBehavior\":\"always\",\"OriginAccessControlOriginType\":\"s3\"}" --region us-east-1
$oacId = ($oac | ConvertFrom-Json).OriginAccessControl.Id

# 2) Create distribution using OAC (use S3 REST endpoint, not website endpoint)
$callerRef = "fakenews-detector-$((Get-Date).Ticks)"
@"
{
  "CallerReference": "$callerRef",
  "Comment": "Fake News Detector Frontend (OAC)",
  "DefaultRootObject": "index.html",
  "Origins": {
    "Quantity": 1,
    "Items": [
      {
        "Id": "S3-fakenews-detector-frontend",
        "DomainName": "fakenews-detector-frontend.s3.amazonaws.com",
        "S3OriginConfig": { "OriginAccessIdentity": "" },
        "OriginAccessControlId": "$oacId"
      }
    ]
  },
  "DefaultCacheBehavior": {
    "TargetOriginId": "S3-fakenews-detector-frontend",
    "ViewerProtocolPolicy": "redirect-to-https",
    "TrustedSigners": { "Enabled": false, "Quantity": 0 },
    "ForwardedValues": { "QueryString": false, "Cookies": { "Forward": "none" } }
  },
  "Enabled": true,
  "PriceClass": "PriceClass_100"
}
"@ | Out-File -FilePath dist-oac.json -Encoding ascii

$dist = aws cloudfront create-distribution --distribution-config file://dist-oac.json --region us-east-1
$distId = ($dist | ConvertFrom-Json).Distribution.Id

# 3) Allow CloudFront distribution to read from S3 via OAC (keep Block Public Access ON)
@"
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowCloudFrontServicePrincipalReadOnly",
      "Effect": "Allow",
      "Principal": { "Service": "cloudfront.amazonaws.com" },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::fakenews-detector-frontend/*",
      "Condition": {
        "StringEquals": {
          "AWS:SourceArn": "arn:aws:cloudfront::979207815314:distribution/$distId"
        }
      }
    }
  ]
}
"@ | Out-File -FilePath bucket-policy-oac.json -Encoding ascii

aws s3api put-bucket-policy --bucket fakenews-detector-frontend --policy file://bucket-policy-oac.json --region ap-southeast-2
```

## Phase 2: Database Setup (DynamoDB)

### Step 1: Create DynamoDB Tables
```bash
# Create articles table
aws dynamodb create-table \
  --table-name fakenews-articles \
  --attribute-definitions \
    AttributeName=id,AttributeType=S \
    AttributeName=analyzed_at,AttributeType=S \
  --key-schema \
    AttributeName=id,KeyType=HASH \
  --global-secondary-indexes \
    IndexName=analyzed-at-index,KeySchema='[{AttributeName=analyzed_at,KeyType=HASH}]',Projection='{ProjectionType=ALL}',ProvisionedThroughput='{ReadCapacityUnits=5,WriteCapacityUnits=5}' \
  --billing-mode PAY_PER_REQUEST

# Create analysis cache table
aws dynamodb create-table \
  --table-name fakenews-analysis-cache \
  --attribute-definitions \
    AttributeName=url_hash,AttributeType=S \
  --key-schema \
    AttributeName=url_hash,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST
```

### Step 2: Migrate Data from SQLite
```python
# Create migration script: migrate_to_dynamodb.py
import sqlite3
import boto3
import hashlib
import json
from datetime import datetime

def migrate_articles():
    # Connect to SQLite
    conn = sqlite3.connect('fakenews/articles.db')
    cursor = conn.cursor()
    
    # Connect to DynamoDB
    dynamodb = boto3.resource('dynamodb')
    articles_table = dynamodb.Table('fakenews-articles')
    
    # Get all articles
    cursor.execute("SELECT * FROM articles")
    articles = cursor.fetchall()
    
    # Migrate each article
    for article in articles:
        item = {
            'id': str(article[0]),
            'url': article[1],
            'title': article[2],
            'content': article[3],
            'source': article[4],
            'domain': article[5],
            'published_at': article[6],
            'analyzed_at': article[7],
            'credibility_score': float(article[8]),
            'language_score': float(article[9]),
            'cross_check_score': float(article[10]),
            'overall_score': float(article[11]),
            'word_count': int(article[12]),
            'sensational_keyword_count': int(article[13]),
            'category': article[14],
            'tfidf_vector': json.loads(article[15]) if article[15] else []
        }
        
        articles_table.put_item(Item=item)
    
    conn.close()
    print(f"Migrated {len(articles)} articles to DynamoDB")

if __name__ == "__main__":
    migrate_articles()
```

## Phase 3: API Setup (Lambda + API Gateway)

### Step 1: Create Lambda Function
```python
# Create lambda_function.py
import json
import boto3
import base64
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
import joblib
import numpy as np
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import nltk
import sys
import os

# Download required NLTK data
nltk.data.path.append('/tmp')
nltk.download('punkt', download_dir='/tmp')
nltk.download('vader_lexicon', download_dir='/tmp')

# Initialize AWS services
dynamodb = boto3.resource('dynamodb')
articles_table = dynamodb.Table('fakenews-articles')
cache_table = dynamodb.Table('fakenews-analysis-cache')

# Load ML models (stored in Lambda layer)
def load_models():
    try:
        model = joblib.load('/opt/sensationalism_model_comprehensive.joblib')
        vectorizer = joblib.load('/opt/tfidf_vectorizer_comprehensive.joblib')
        scaler = joblib.load('/opt/scaler_comprehensive.joblib')
        return model, vectorizer, scaler
    except Exception as e:
        print(f"Error loading models: {e}")
        return None, None, None

# Initialize models
model, vectorizer, scaler = load_models()
sentiment_analyzer = SentimentIntensityAnalyzer()

def lambda_handler(event, context):
    try:
        # Parse request
        if event.get('httpMethod') == 'POST':
            body = json.loads(event['body'])
            url = body.get('url')
        else:
            url = event.get('url')
        
        if not url:
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json'},
                'body': json.dumps({'error': 'URL is required'})
            }
        
        # Check cache first
        url_hash = hashlib.md5(url.encode()).hexdigest()
        cache_response = cache_table.get_item(Key={'url_hash': url_hash})
        
        if 'Item' in cache_response:
            return {
                'statusCode': 200,
                'headers': {'Content-Type': 'application/json'},
                'body': json.dumps({
                    'success': True,
                    'cached': True,
                    'article': cache_response['Item']['analysis_result']
                })
            }
        
        # Extract article content
        article_data = extract_article(url)
        if not article_data:
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json'},
                'body': json.dumps({'error': 'Failed to extract article content'})
            }
        
        # Analyze article
        analysis_result = analyze_article(article_data)
        
        # Cache result
        cache_table.put_item(Item={
            'url_hash': url_hash,
            'url': url,
            'analysis_result': analysis_result,
            'timestamp': datetime.now().isoformat()
        })
        
        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json'},
            'body': json.dumps({
                'success': True,
                'cached': False,
                'article': analysis_result
            })
        }
        
    except Exception as e:
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json'},
            'body': json.dumps({'error': str(e)})
        }

def extract_article(url):
    # Article extraction logic (simplified)
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title = soup.find('title').text if soup.find('title') else 'No title'
        content = ' '.join([p.text for p in soup.find_all('p')])
        
        return {
            'url': url,
            'title': title,
            'content': content,
            'source': urlparse(url).netloc
        }
    except Exception as e:
        print(f"Error extracting article: {e}")
        return None

def analyze_article(article_data):
    # Simplified analysis (implement full logic from app.py)
    if model and vectorizer and scaler:
        # Use ML models for analysis
        text = article_data['content']
        tfidf_features = vectorizer.transform([text]).toarray()
        
        # Add enhanced features
        enhanced_features = extract_enhanced_features(text)
        combined_features = np.concatenate([tfidf_features, enhanced_features], axis=1)
        scaled_features = scaler.transform(combined_features)
        
        sensationalism_score = model.predict_proba(scaled_features)[0][1]
    else:
        # Fallback analysis
        sensationalism_score = 0.5
    
    return {
        'id': hashlib.md5(article_data['url'].encode()).hexdigest(),
        'url': article_data['url'],
        'title': article_data['title'],
        'content': article_data['content'],
        'source': article_data['source'],
        'domain': urlparse(article_data['url']).netloc,
        'published_at': None,
        'analyzed_at': datetime.now().isoformat(),
        'credibility_score': 0.7,
        'language_score': 0.8,
        'cross_check_score': 0.5,
        'overall_score': sensationalism_score,
        'word_count': len(article_data['content'].split()),
        'sensational_keyword_count': 0,
        'category': None,
        'tfidf_vector': [],
        'sensationalism_score': sensationalism_score,
        'word_frequency': analyze_word_frequency(article_data['content']),
        'sentiment_flow': analyze_sentiment_flow(article_data['content'])
    }

def extract_enhanced_features(text):
    # Simplified feature extraction
    return np.array([[0.1, 0.2, 0.3, 0.4, 0.5]])  # Placeholder

def analyze_word_frequency(text):
    # Word frequency analysis
    return {
        'credible_words': {'news': 2, 'report': 1},
        'suspicious_words': {'breaking': 1, 'shocking': 1}
    }

def analyze_sentiment_flow(text):
    # Sentiment flow analysis
    sentences = text.split('.')
    return [
        {'sentence': sent.strip(), 'sentiment_score': 0.1}
        for sent in sentences[:5]
    ]
```

### Step 2: Create Lambda Layer for ML Models
```bash
# Create layer directory
mkdir lambda-layer
cd lambda-layer
mkdir python

# Copy ML models and dependencies
cp fakenews/models/*.joblib python/
pip install joblib scikit-learn nltk vaderSentiment -t python/

# Create layer zip
zip -r ml-models-layer.zip python/

# Upload layer
aws lambda publish-layer-version \
  --layer-name fakenews-ml-models \
  --zip-file fileb://ml-models-layer.zip \
  --compatible-runtimes python3.11
```

### Step 3: Deploy Lambda Function
```bash
# Create deployment package
zip -r lambda-deployment.zip lambda_function.py

# Create Lambda function
aws lambda create-function \
  --function-name fakenews-analyzer \
  --runtime python3.11 \
  --role arn:aws:iam::YOUR_ACCOUNT:role/lambda-execution-role \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://lambda-deployment.zip \
  --timeout 30 \
  --memory-size 1024 \
  --layers arn:aws:lambda:REGION:YOUR_ACCOUNT:layer:fakenews-ml-models:1
```

### Step 4: Create API Gateway
```bash
# Create REST API
aws apigateway create-rest-api \
  --name fakenews-api \
  --description "Fake News Detector API"

# Get API ID
API_ID=$(aws apigateway get-rest-apis --query 'items[?name==`fakenews-api`].id' --output text)

# Create resource
aws apigateway create-resource \
  --rest-api-id $API_ID \
  --parent-id $(aws apigateway get-resources --rest-api-id $API_ID --query 'items[?path==`/`].id' --output text) \
  --path-part analyze

# Create method
aws apigateway put-method \
  --rest-api-id $API_ID \
  --resource-id $(aws apigateway get-resources --rest-api-id $API_ID --query 'items[?pathPart==`analyze`].id' --output text) \
  --http-method POST \
  --authorization-type NONE

# Set up Lambda integration
aws apigateway put-integration \
  --rest-api-id $API_ID \
  --resource-id $(aws apigateway get-resources --rest-api-id $API_ID --query 'items[?pathPart==`analyze`].id' --output text) \
  --http-method POST \
  --type AWS_PROXY \
  --integration-http-method POST \
  --uri arn:aws:apigateway:REGION:lambda:path/2015-03-31/functions/arn:aws:lambda:REGION:YOUR_ACCOUNT:function:fakenews-analyzer/invocations

# Deploy API
aws apigateway create-deployment \
  --rest-api-id $API_ID \
  --stage-name prod
```

## Phase 4: ML Storage Setup (S3)

### Step 1: Create S3 Bucket for ML Models
```bash
# Create bucket for ML models
aws s3 mb s3://fakenews-ml-models

# Upload ML models
aws s3 cp fakenews/models/ s3://fakenews-ml-models/ --recursive

# Set versioning
aws s3api put-bucket-versioning \
  --bucket fakenews-ml-models \
  --versioning-configuration Status=Enabled
```

### Step 2: Create S3 Bucket for Data Storage
```bash
# Create bucket for data storage
aws s3 mb s3://fakenews-data-storage

# Create folder structure
aws s3api put-object --bucket fakenews-data-storage --key articles/
aws s3api put-object --bucket fakenews-data-storage --key analysis-cache/
aws s3api put-object --bucket fakenews-data-storage --key backups/
```

## Phase 5: Authentication Setup (API Gateway + Cognito)

### Step 1: Create Cognito User Pool
```bash
# Create user pool
aws cognito-idp create-user-pool \
  --pool-name fakenews-users \
  --policies '{
    "PasswordPolicy": {
      "MinimumLength": 8,
      "RequireUppercase": true,
      "RequireLowercase": true,
      "RequireNumbers": true,
      "RequireSymbols": false
    }
  }' \
  --auto-verified-attributes email

# Create user pool client
aws cognito-idp create-user-pool-client \
  --user-pool-id YOUR_USER_POOL_ID \
  --client-name fakenews-web-client \
  --generate-secret \
  --explicit-auth-flows USER_PASSWORD_AUTH
```

### Step 2: Configure API Gateway Authorization
```bash
# Create authorizer
aws apigateway create-authorizer \
  --rest-api-id $API_ID \
  --name cognito-authorizer \
  --type COGNITO_USER_POOLS \
  --provider-arns arn:aws:cognito-idp:REGION:YOUR_ACCOUNT:userpool/YOUR_USER_POOL_ID

# Update method to require authorization
aws apigateway update-method \
  --rest-api-id $API_ID \
  --resource-id $(aws apigateway get-resources --rest-api-id $API_ID --query 'items[?pathPart==`analyze`].id' --output text) \
  --http-method POST \
  --authorization-type COGNITO_USER_POOLS \
  --authorizer-id $(aws apigateway get-authorizers --rest-api-id $API_ID --query 'items[?name==`cognito-authorizer`].id' --output text)
```

## Phase 6: Monitoring and Logging

### Step 1: Setup CloudWatch Logs
```bash
# Create log group
aws logs create-log-group \
  --log-group-name /aws/lambda/fakenews-analyzer

# Set retention policy
aws logs put-retention-policy \
  --log-group-name /aws/lambda/fakenews-analyzer \
  --retention-in-days 14
```

### Step 2: Setup CloudWatch Alarms
```bash
# Create error rate alarm
aws cloudwatch put-metric-alarm \
  --alarm-name fakenews-lambda-errors \
  --alarm-description "Lambda function errors" \
  --metric-name Errors \
  --namespace AWS/Lambda \
  --statistic Sum \
  --period 300 \
  --threshold 5 \
  --comparison-operator GreaterThanThreshold \
  --dimensions Name=FunctionName,Value=fakenews-analyzer \
  --evaluation-periods 2
```

## Phase 7: Cost Optimization

### Step 1: Setup S3 Lifecycle Policies
```bash
# Create lifecycle policy for data storage
aws s3api put-bucket-lifecycle-configuration \
  --bucket fakenews-data-storage \
  --lifecycle-configuration '{
    "Rules": [
      {
        "ID": "ArchiveOldData",
        "Status": "Enabled",
        "Transitions": [
          {
            "Days": 30,
            "StorageClass": "STANDARD_IA"
          },
          {
            "Days": 90,
            "StorageClass": "GLACIER"
          }
        ]
      }
    ]
  }'
```

### Step 2: Setup DynamoDB Auto Scaling
```bash
# Enable auto scaling for articles table
aws application-autoscaling register-scalable-target \
  --service-namespace dynamodb \
  --resource-id table/fakenews-articles \
  --scalable-dimension dynamodb:table:ReadCapacityUnits \
  --min-capacity 5 \
  --max-capacity 50

# Create scaling policy
aws application-autoscaling put-scaling-policy \
  --service-namespace dynamodb \
  --resource-id table/fakenews-articles \
  --scalable-dimension dynamodb:table:ReadCapacityUnits \
  --policy-name fakenews-read-scaling \
  --policy-type TargetTrackingScaling \
  --target-tracking-scaling-policy-configuration '{
    "TargetValue": 70.0,
    "ScaleInCooldown": 60,
    "ScaleOutCooldown": 60
  }'
```

## Phase 8: Security Configuration

### Step 1: Setup IAM Roles and Policies
```bash
# Create Lambda execution role
aws iam create-role \
  --role-name fakenews-lambda-role \
  --assume-role-policy-document '{
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Principal": {
          "Service": "lambda.amazonaws.com"
        },
        "Action": "sts:AssumeRole"
      }
    ]
  }'

# Attach policies
aws iam attach-role-policy \
  --role-name fakenews-lambda-role \
  --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

aws iam attach-role-policy \
  --role-name fakenews-lambda-role \
  --policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess

aws iam attach-role-policy \
  --role-name fakenews-lambda-role \
  --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
```

### Step 2: Setup VPC (Optional)
```bash
# Create VPC for Lambda (if needed for RDS access)
aws ec2 create-vpc \
  --cidr-block 10.0.0.0/16 \
  --tag-specifications 'ResourceType=vpc,Tags=[{Key=Name,Value=fakenews-vpc}]'

# Create subnets, security groups, etc.
# (Detailed VPC setup omitted for brevity)
```

## Phase 9: Deployment and Testing

### Step 1: Update Frontend Configuration
```javascript
// Update static/js/api.js
const API_BASE_URL = 'https://YOUR_API_GATEWAY_URL/prod';

// Update API endpoints
const API_ENDPOINTS = {
    analyze: `${API_BASE_URL}/analyze`,
    predict: `${API_BASE_URL}/predict`,
    health: `${API_BASE_URL}/health`
};
```

### Step 2: Deploy and Test
```bash
# Deploy frontend updates
aws s3 sync fakenews/static/ s3://fakenews-detector-frontend --delete

# Invalidate CloudFront cache
aws cloudfront create-invalidation \
  --distribution-id YOUR_DISTRIBUTION_ID \
  --paths "/*"

# Test API endpoint
curl -X POST https://YOUR_API_GATEWAY_URL/prod/analyze \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/article"}'
```

## Phase 10: Production Optimization

### Step 1: Setup CI/CD Pipeline
```yaml
# Create .github/workflows/deploy.yml
name: Deploy to AWS
on:
  push:
    branches: [main]
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Deploy to S3
        run: aws s3 sync static/ s3://fakenews-detector-frontend --delete
      - name: Invalidate CloudFront
        run: aws cloudfront create-invalidation --distribution-id ${{ secrets.CLOUDFRONT_ID }} --paths "/*"
```

### Step 2: Setup Monitoring Dashboard
```bash
# Create CloudWatch dashboard
aws cloudwatch put-dashboard \
  --dashboard-name fakenews-monitoring \
  --dashboard-body '{
    "widgets": [
      {
        "type": "metric",
        "properties": {
          "metrics": [
            ["AWS/Lambda", "Invocations", "FunctionName", "fakenews-analyzer"],
            ["AWS/Lambda", "Errors", "FunctionName", "fakenews-analyzer"],
            ["AWS/Lambda", "Duration", "FunctionName", "fakenews-analyzer"]
          ],
          "period": 300,
          "stat": "Sum",
          "region": "us-east-1",
          "title": "Lambda Metrics"
        }
      }
    ]
  }'
```

## Estimated Monthly Costs

### Small Scale (< 1000 users/month)
- S3 Storage: $0-5
- CloudFront: $0-5
- Lambda: $0-10
- DynamoDB: $0-25
- API Gateway: $0-3.50
- **Total: $0-48.50/month**

### Medium Scale (1000-10000 users/month)
- S3 Storage: $5-20
- CloudFront: $5-50
- Lambda: $10-50
- DynamoDB: $25-100
- API Gateway: $3.50-35
- **Total: $48.50-255/month**

### Large Scale (> 10000 users/month)
- S3 Storage: $20-100
- CloudFront: $50-500
- Lambda: $50-200
- DynamoDB: $100-500
- API Gateway: $35-350
- **Total: $255-1650/month**

## Security Best Practices

1. **Enable MFA** on AWS account
2. **Use IAM roles** instead of access keys
3. **Enable CloudTrail** for audit logging
4. **Use VPC endpoints** for private communication
5. **Enable encryption** at rest and in transit
6. **Regular security audits** and updates
7. **Monitor costs** with AWS Budgets
8. **Backup data** regularly to S3 Glacier

## Troubleshooting Common Issues

### Lambda Cold Starts
- Use provisioned concurrency for critical functions
- Optimize package size
- Use Lambda layers for dependencies

### DynamoDB Throttling
- Enable auto-scaling
- Use exponential backoff
- Consider on-demand billing

### API Gateway Limits
- Use request validation
- Implement rate limiting
- Monitor usage patterns

### S3 Access Issues
- Check bucket policies
- Verify CORS configuration
- Review IAM permissions

## Next Steps

1. **Test all components** individually
2. **Load test** the complete system
3. **Monitor performance** and costs
4. **Implement backup** and disaster recovery
5. **Set up alerts** for critical issues
6. **Document** operational procedures
7. **Train team** on AWS services
8. **Plan scaling** strategy for growth
